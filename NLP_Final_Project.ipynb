{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Final-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PLdIdCNfQGcr",
        "GUNpA2ORMcu3",
        "x1gUN3TlNAGN",
        "QiQpZN2rNGNw",
        "mwMCRQn-OR2_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preyansh98/NLP_Final_Project/blob/main/NLP_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4TiwpkhPsc7"
      },
      "source": [
        "**Introduction**\n",
        "\n",
        "*Multi-label classification is a process by which the properties of data-points that are not mutually exclusive are predicted. Each sample of data is categorized with a target label. Multi-label text classification can be utilized in a multitude of applications, such as social media targeting, recognizing opinions and sentiments and building recommendation systems. Most applications of multi-label text classification do not consider the effect that word order can have on the performance of their models. Our paper therefore aims to evaluate the performance of multi-label learning by comparing the three following pre-training techniques: bag-of-words, word2vec with consideration of word order, and ELMo which takes word context into account. Specifically, these pre-training techniques will be applied independently as feature extraction techniques, and then subsequently evaluated through a Binary Relevance across models such as Logistic Regression, Multinomial Na√Øve Bayes, and SVM, to solve the multi-label text classification of predicting movie genres based on their plot summaries.*\n",
        "\n",
        "Project by\n",
        "\n",
        "*   Preyansh Kaushik - 260790402\n",
        "*   Elie Elia - 260759306\n",
        "*   Rozerin Akkus  - 260775633\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WG5o1cINC-_"
      },
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib.pyplot as plt \n",
        "import nltk\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/COMP550/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLdIdCNfQGcr"
      },
      "source": [
        "# Load Datasets\n",
        "\n",
        "We will load the movie dataset corpus for our experiment from CMU. \n",
        "This includes two files:\n",
        "\n",
        "*   plot_summaries.txt : Movie ID and the plot summaries for the movies\n",
        "*   movie.metadata.tsv : Tab-separated values for movie ID, name, genre\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKXVdo0DQ-qo"
      },
      "source": [
        "path_to_movie_dataset = path + \"movie.metadata.tsv\"\n",
        "path_to_plot_summaries = path + \"plot_summaries.txt\"\n",
        "\n",
        "# headers obtained from corpus website\n",
        "headers = [\"movie_id\", \"freebase_id\", \"movie_name\", \"movie_release_date\", \"movie_box_office_rev\", \"movie_runtime\", \"movie_langs\", \"movie_countries\", \"genres\"]\n",
        "\n",
        "metadata = pd.read_csv(path_to_movie_dataset, sep = \"\\t\", names = headers)\n",
        "print(\"Movie IDs available \", metadata.shape[0])\n",
        "metadata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjordavtTU6v"
      },
      "source": [
        "movie_ids, plots = [], []\n",
        "\n",
        "with open(path_to_plot_summaries, \"rt\") as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "  for line in lines:\n",
        "    data = line.split(None, 1)\n",
        "\n",
        "    movie_ids.append(data[0])\n",
        "    plots.append(data[1])\n",
        "\n",
        "movies_and_plots = pd.DataFrame({'movie_id': movie_ids, 'plot_summary' : plots})\n",
        "movies_and_plots.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPqAZvBgl14D"
      },
      "source": [
        "Now, we merge the two datasets. \n",
        "\n",
        "We only need movie_id, movie name, plot_summary, and genres. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TItDCeJonMry"
      },
      "source": [
        "metadata['movie_id'] = metadata['movie_id'].astype(str)\n",
        "\n",
        "dataset = pd.merge(movies_and_plots, metadata, on='movie_id')\n",
        "\n",
        "# remove unnescessary columns\n",
        "dataset = dataset[set(['movie_id','plot_summary', 'movie_name', 'genres'])]\n",
        "\n",
        "# reorder columns\n",
        "dataset = dataset[['movie_id', 'movie_name', 'plot_summary', 'genres']]\n",
        "\n",
        "# extract genres\n",
        "genres_column = []\n",
        "\n",
        "for val in dataset['genres']:\n",
        "  genres = list(json.loads(val).values())\n",
        "  genres_column.append(genres)\n",
        "\n",
        "dataset['genres'] = genres_column\n",
        "print(dataset.shape)\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUNpA2ORMcu3"
      },
      "source": [
        "# Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1gUN3TlNAGN"
      },
      "source": [
        "## Cleaning Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuScZo3GbgcE"
      },
      "source": [
        "# remove those with no genres from dataset.\r\n",
        "\r\n",
        "rowsToDelete = []\r\n",
        "for i in range(len(dataset)):\r\n",
        "  if (len(dataset['genres'][i]) == 0):\r\n",
        "      rowsToDelete.append(i)\r\n",
        "\r\n",
        "# print(len(rowsToDelete))\r\n",
        "\r\n",
        "dataset.drop(index = rowsToDelete, inplace = True)\r\n",
        "# print(dataset.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiQpZN2rNGNw"
      },
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "- Remove stopwords, lemmatize, etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK4jNx5nNJYd"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def clean_sentence(sentence):\n",
        "  sentence = sentence.lower()\n",
        "\n",
        "  # remove whitespaces\n",
        "  sentence = re.sub(\"\\\\W\", \" \", sentence)\n",
        "\n",
        "  # remove non-alphabets\n",
        "  sentence = re.sub(\"[^a-zA-Z]\",\" \",sentence)\n",
        "\n",
        "  return sentence\n",
        "\n",
        "def remove_stopwords(sentence):\n",
        "  return ' '.join([word for word in sentence.split() if not word in stop_words])\n",
        "\n",
        "def lemmatize_text(sentence):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  lemmatized_words = []\n",
        "\n",
        "  for word, tag in pos_tag(word_tokenize(sentence)):\n",
        "    wntag = tag[0].lower()\n",
        "    wntag = wntag if wntag in ['a','r','n','v'] else None\n",
        "\n",
        "    if wntag:\n",
        "      lemmatized_words.append(lemmatizer.lemmatize(word,wntag))\n",
        "    else:\n",
        "      lemmatized_words.append(word)\n",
        "\n",
        "  return ' '.join(lemmatized_words)\n",
        "\n",
        "def stem_text(sentence):\n",
        "  stemmer = PorterStemmer()\n",
        "  return ' '.join([stemmer.stem(word = word) for word in sentence])\n",
        "\n",
        "\"\"\" Takes a pandas data column, and applies a series of functions to each value. Returns pandas column \"\"\"\n",
        "def clean_data(data_column, rm_stopwords, lemm, stem = False):\n",
        "  if lemm and stem:\n",
        "    raise \"Either lemmatize or stem. Both can not be true.\"\n",
        "\n",
        "  result = data_column\n",
        "\n",
        "  # basic preprocessing, remove punctuation etc. \n",
        "  result = result.apply(lambda sentence : clean_sentence(sentence))\n",
        "\n",
        "  if rm_stopwords:\n",
        "    result = result.apply(lambda sentence : remove_stopwords(sentence))\n",
        "\n",
        "  if lemm:\n",
        "    result = result.apply(lambda sentence : lemmatize_text(sentence))\n",
        "\n",
        "  if stem:\n",
        "    result = result.apply(lambda sentence : stem_text(sentence))\n",
        "\n",
        "  return result\n",
        "\n",
        "dataset['plot_summary_cleaned'] = clean_data(dataset['plot_summary'], rm_stopwords = False, lemm = False, stem = False)\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwMCRQn-OR2_"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "-- Visualize features of our dataset\n",
        "    (word clouds for all genres in a dropdown) AND (frequency of genres)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ub1lL79tUIY"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "import ipywidgets as wid\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def filter_dataset_by_genre(genres_list):\n",
        "  return dataset[pd.DataFrame(dataset.genres.tolist()).isin(genres_list).any(1).values]\n",
        "\n",
        "def get_words_for_genre(genre):\n",
        "  plots = filter_dataset_by_genre([genre])['plot_summary']\n",
        "  return plots\n",
        "\n",
        "def plot_word_map_for_genre(dataset, title, genre):\n",
        "  plots = get_words_for_genre(genre)\n",
        "  words = ''\n",
        "  for plot in plots:\n",
        "    tokens = wordpunct_tokenize(plot)\n",
        "    for i in range(len(tokens)):\n",
        "      tokens[i]   = tokens[i].lower()\n",
        "    words += \" \".join(tokens)+\" \"\n",
        "\n",
        "  wordcloud = WordCloud(width = 800, height = 800, \n",
        "                  background_color ='white', \n",
        "                  stopwords = stop_words, \n",
        "                  min_font_size = 10).generate(words) \n",
        "                  \n",
        "  plt.figure(figsize = (8, 8), facecolor = None) \n",
        "  plt.imshow(wordcloud) \n",
        "  plt.axis(\"off\") \n",
        "  plt.tight_layout(pad = 0) \n",
        "    \n",
        "  plt.show()\n",
        "\n",
        "def event_handler(index):\n",
        "  for i in range(10):\n",
        "    clear_output(wait=True)\n",
        "  dropdown_menu(index.new)\n",
        "  plot_word_map_for_genre(dataset, title=\"Word Map for {} genre\".format(index.new), genre=index.new)\n",
        "\n",
        "def dropdown_menu(value):\n",
        "  dropdown = wid.Dropdown(options = sorted(list(set([item for sublist in dataset['genres'].tolist() for item in sublist]))), value=value)\n",
        "  dropdown.observe(event_handler, names=\"value\")\n",
        "  display(dropdown)\n",
        "\n",
        "dropdown_menu('Drama')\n",
        "plot_word_map_for_genre(dataset, title=\"Word Map for {} genre\".format('Drama'), genre='Drama')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO0G-arhfCl-"
      },
      "source": [
        "import seaborn as sns\r\n",
        "\r\n",
        "genres_list  = sum(dataset['genres'], [])\r\n",
        "# len(set(genres_list))\r\n",
        "\r\n",
        "genres_list  = nltk.FreqDist(genres_list)\r\n",
        "freq_df = pd.DataFrame({'Genre name': list(genres_list.keys()), 'Frequency': list([(x/sum(genres_list.values()))*100 for x in genres_list.values()])})\r\n",
        "\r\n",
        "freq_g = freq_df.nlargest(columns = \"Frequency\", n = 40)\r\n",
        "plt.figure(figsize = (12,6))\r\n",
        "ax = sns.barplot(data = freq_g,  x = \"Genre name\", y =\"Frequency\", palette='Reds_r')\r\n",
        "ax.set(xlabel='Genres' , ylabel='Frequency in %')\r\n",
        "plt.xticks(rotation = 90)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t74SHpwM7Po"
      },
      "source": [
        "# Feature Generation\n",
        "\n",
        "Vectorize data to features using different techniques:\n",
        "- TF-IDF (bag-of-words)\n",
        "- word2vec\n",
        "- ElMO "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GoVcnEdzh6t"
      },
      "source": [
        "First, build input X and output Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsxkqfu91fYB"
      },
      "source": [
        "X = dataset['plot_summary_cleaned'] \n",
        "Y = dataset['genres']\n",
        "\n",
        "all_genres = sorted(list(set([item for sublist in dataset['genres'].tolist() for item in sublist])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4QI_0ttaMUy"
      },
      "source": [
        "Split data into train-valid-test, and one-hot encode the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2PmCgfraQpE"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
        "\n",
        "mlb = MultiLabelBinarizer().fit(Y)\n",
        "\n",
        "y_train_ohe = mlb.transform(y_train)\n",
        "y_test_ohe = mlb.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-Vof1UH39RF"
      },
      "source": [
        "### TF-IDF:\n",
        "https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOKB1ng3NKID"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "TF_VOCAB_SIZE = 10000\n",
        "\n",
        "tf_vec = TfidfVectorizer(max_features=TF_VOCAB_SIZE)\n",
        "tf_feature_train = tf_vec.fit_transform(x_train)\n",
        "tf_feature_test = tf_vec.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaHSqJQT4A9C"
      },
      "source": [
        "### Word2Vec:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lqC82T9-Gab"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "from time import time \r\n",
        "import logging \r\n",
        "logging.basicConfig(format = \"%(levelname)s-%(asctime)s: %(message)s\", datefmt = '%H:%M:%S', level = logging.INFO)\r\n",
        "\r\n",
        "tokenized = []\r\n",
        "for  i  in range(len(dataset)):\r\n",
        "  row   = dataset.iloc[i]\r\n",
        "  token_tagged_plot = row['plot_summary_cleaned'].split()+row['genres']\r\n",
        "  tokenized.append(token_tagged_plot)\r\n",
        "\r\n",
        "print(len(tokenized))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGrPUUXZPBQm"
      },
      "source": [
        "w2vec_model = Word2Vec(tokenized, \r\n",
        "                        min_count = 3,\r\n",
        "                        window  = 10,\r\n",
        "                        size  = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68ol5Sf0PF6F"
      },
      "source": [
        "t = time()\r\n",
        "\r\n",
        "w2vec_model.train(tokenized, total_examples=w2vec_model.corpus_count, epochs=20, report_delay=1)\r\n",
        "\r\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdT8sRFnPSUE"
      },
      "source": [
        "w2vec_model.wv.init_sims(replace = True)\r\n",
        "w2vec_model.wv.most_similar(positive = ['Musical'], topn = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9h4JVbePZ-f"
      },
      "source": [
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "\r\n",
        "def tsne_plot(model, word, list_names):\r\n",
        "  arrays = np.empty((0, 150), dtype='f')\r\n",
        "  word_labels = [word]\r\n",
        "  color_list  = ['red']\r\n",
        "\r\n",
        "  arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\r\n",
        "    \r\n",
        "    # gets list of most similar words\r\n",
        "  close_words = model.wv.most_similar([word], topn = 20)\r\n",
        "  \r\n",
        "  # adds the vector for each of the closest words to the array\r\n",
        "  for wrd_score in close_words:\r\n",
        "      wrd_vector = model.wv.__getitem__([wrd_score[0]])\r\n",
        "      word_labels.append(wrd_score[0])\r\n",
        "      color_list.append('blue')\r\n",
        "      arrays = np.append(arrays, wrd_vector, axis=0)\r\n",
        "  \r\n",
        "  for wrd in list_names:\r\n",
        "      wrd_vector = model.wv.__getitem__([wrd])\r\n",
        "      word_labels.append(wrd)\r\n",
        "      color_list.append('green')\r\n",
        "      arrays = np.append(arrays, wrd_vector, axis=0)\r\n",
        "        \r\n",
        "    # Reduces the dimensionality from 150 to 25 dimensions with PCA\r\n",
        "  reduc = PCA(n_components=25).fit_transform(arrays)\r\n",
        "  \r\n",
        "  # Finds t-SNE coordinates for 2 dimensions\r\n",
        "  np.set_printoptions(suppress=True)\r\n",
        "  \r\n",
        "  Y = TSNE(n_components=2, random_state=0, perplexity=20).fit_transform(reduc)\r\n",
        "  \r\n",
        "  # Sets everything up to plot\r\n",
        "  df = pd.DataFrame({'x': [x for x in Y[:, 0]],\r\n",
        "                      'y': [y for y in Y[:, 1]],\r\n",
        "                      'words': word_labels,\r\n",
        "                      'color': color_list})\r\n",
        "  \r\n",
        "  fig, _ = plt.subplots()\r\n",
        "\r\n",
        "  p1 = sns.regplot(data=df,\r\n",
        "                     x=\"x\",\r\n",
        "                     y=\"y\",\r\n",
        "                     fit_reg=False,\r\n",
        "                     marker=\"o\",\r\n",
        "                     scatter_kws={'s': 40,\r\n",
        "                                  'facecolors': df['color']\r\n",
        "                                 }\r\n",
        "                    )\r\n",
        "    \r\n",
        "  for line in range(0, df.shape[0]):\r\n",
        "        p1.text(df[\"x\"][line],\r\n",
        "                df['y'][line],\r\n",
        "                '  ' + df[\"words\"][line].title(),\r\n",
        "                horizontalalignment='left',\r\n",
        "                verticalalignment='bottom', size='small',\r\n",
        "                color=df['color'][line],\r\n",
        "                weight='normal'\r\n",
        "              ).set_size(20)\r\n",
        "\r\n",
        "  \r\n",
        "  plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\r\n",
        "  plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\r\n",
        "          \r\n",
        "  plt.title('t-SNE visualization for {}'.format(word.title()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWu33eJcPeTy"
      },
      "source": [
        "# testing out with top 20 similar words vs. random words \r\n",
        "tsne_plot(w2vec_model, 'Musical', ['dog', 'cat', 'coffee', 'computer', 'table', 'bird'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVx9a4UiPivj"
      },
      "source": [
        "tsne_plot(w2vec_model, 'love', ['dog', 'cat', 'coffee', 'computer', 'table', 'bird'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ub7c_1YE0VG"
      },
      "source": [
        "def buildWordVector(text, size):\n",
        "  vec = np.zeros(size).reshape((1, size))\n",
        "  count = 0.\n",
        "  word = text.split()\n",
        "  for word in text:\n",
        "      try:\n",
        "          vec += w2vec_model[word].reshape((1, size))\n",
        "          count += 1.\n",
        "          \n",
        "      except KeyError:\n",
        "          continue\n",
        "  if count != 0:\n",
        "      vec /= count\n",
        "  return vec\n",
        "\n",
        "from sklearn.preprocessing import scale\n",
        "w2v_feature_train = np.concatenate([buildWordVector(z, 100) for z in x_train])\n",
        "w2v_feature_train = scale(w2v_feature_train)\n",
        "# w2vec_model.train(x_test)\n",
        "\n",
        "w2v_feature_test = np.concatenate([buildWordVector(z, 100) for z in x_test])\n",
        "w2v_feature_test = scale(w2v_feature_test)\n",
        "\n",
        "y_train_w2v = [i[0] for i in y_train]\n",
        "y_test_w2v = [i[0] for i in y_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9teUV3M-4CXr"
      },
      "source": [
        "### ElMO:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkhXqCOFcG0G"
      },
      "source": [
        "save = False\n",
        "\n",
        "if save:\n",
        "  pickle_out = open(\"elmo_train.pickle\",\"wb\")\n",
        "  pickle.dump(elmo_feature_train, pickle_out)\n",
        "  pickle_out.close()\n",
        "\n",
        "  # save elmo_test_new\n",
        "  pickle_out = open(\"elmo_test.pickle\",\"wb\")\n",
        "  pickle.dump(elmo_feature_test, pickle_out)\n",
        "  pickle_out.close()\n",
        "\n",
        "else:\n",
        "  pickle_in = open(\"elmo_train.pickle\", \"rb\")\n",
        "  elmo_feature_train = pickle.load(pickle_in)\n",
        "\n",
        "# load elmo_train_new\n",
        "  pickle_in = open(\"elmo_test.pickle\", \"rb\")\n",
        "  elmo_feature_test = pickle.load(pickle_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eYYmPNq4DOI"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "url = \"https://tfhub.dev/google/elmo/2\"\n",
        "elmo = hub.Module(url)\n",
        "\n",
        "# we will define a function to generate elmo embeddings:\n",
        "def get_elmo_embeddings(x):\n",
        "  embeddings = elmo(x, signature=\"default\", as_dict=True)['elmo']\n",
        "\n",
        "  with tf.Session() as session:\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "\n",
        "    return session.run(tf.reduce_mean(embeddings,1))\n",
        "\n",
        "# Since elmo takes a long time, we'll train sequentially in batches of a 100 samples. \n",
        "train_batches = [x_train[i:i+50] for i in range(0, x_train.shape[0], 50)]\n",
        "test_batches = [x_test[i:i+50] for i in range(0, x_test.shape[0],50)]\n",
        "\n",
        "elmo_train_embeddings = [get_elmo_embeddings(batch) for batch in train_batches]\n",
        "elmo_test_embeddings = [get_elmo_embeddings(batch) for batch in test_batches]\n",
        "\n",
        "# concatenate\n",
        "elmo_feature_train = np.concatenate(elmo_train_embeddings, axis=0)\n",
        "elmo_feature_test = np.concatenate(elmo_test_embeddings, axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt_MOD__PMBA"
      },
      "source": [
        "# Models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbLghVG01ZoQ"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "THRESHOLD = 0.3\n",
        "\n",
        "# take in a model, and parameters, then grid search and fit. \n",
        "def run_cv_fit_model(model, parameters, x_train, y_train):\n",
        "  classifier_cv = OneVsRestClassifier(model)\n",
        "\n",
        "  # tune hyperparameters with CV:\n",
        "  classifier_cv = GridSearchCV(classifier, parameters, cv=5)\n",
        "\n",
        "  return classifier_cv.fit(x_train, y_train)\n",
        "\n",
        "# def inverse one-hot encode output to the genres\n",
        "def inverse_to_genres(pred):\n",
        "  return mlb.inverse_transform(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn113xqQSR4s"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pQehFcVSS8D"
      },
      "source": [
        "class LogisticReg:\n",
        "  def __init__(self, param_grid, x_train, y_train):\n",
        "    self.model = self.__compile__(param_grid, x_train, y_train)\n",
        "    return\n",
        "\n",
        "  def __compile__(self, param_grid, x_train, y_train):\n",
        "    lr = LogisticRegression(penalty='l2')\n",
        "    return run_cv_fit_model(lr, param_grid, x_train, y_train)\n",
        "\n",
        "  def predict(self, x_test):\n",
        "    return self.model.predict(x_test)\n",
        "\n",
        "  def score(self, x_test, y_test):\n",
        "    return self.model.score(x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjUFf9ZVW5wZ"
      },
      "source": [
        "### Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZHxwE0QW7gU"
      },
      "source": [
        "class MNB:\n",
        "  def __init__(self, param_grid, x_train, y_train):\n",
        "    self.model = self.__compile__(param_grid, x_train, y_train)\n",
        "    return\n",
        "\n",
        "  def __compile__(self, param_grid, x_train, y_train):\n",
        "    mnb = MultinomialNB()\n",
        "    return run_cv_fit_model(mnb, param_grid, x_train, y_train)\n",
        "\n",
        "  def predict(self, x_test):\n",
        "    return self.model.predict(x_test)\n",
        "\n",
        "  def score(self, x_test, y_test):\n",
        "    return self.model.score(x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7BxMXa5XJkZ"
      },
      "source": [
        "### SVM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmbAkcF8XKmO"
      },
      "source": [
        "class SVM:\n",
        "  def __init__(self, param_grid, x_train, y_train):\n",
        "    self.model = self.__compile__(param_grid, x_train, y_train)\n",
        "    return\n",
        "\n",
        "  def __compile__(self, param_grid, x_train, y_train):\n",
        "    model = SVC(kernel='linear')\n",
        "    return run_cv_fit_model(model, param_grid, x_train, y_train)\n",
        "\n",
        "  def predict(self, x_test):\n",
        "    return self.model.predict(x_test)\n",
        "\n",
        "  def score(self, x_test, y_test):\n",
        "    return self.model.score(x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_psjWNK1Nc4"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OusuWdmY71sf"
      },
      "source": [
        "## Logistic Regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1rF6VnOTwQG"
      },
      "source": [
        "# define parameter grid for logistic regression\n",
        "C = np.logspace(0,4,10)\n",
        "\n",
        "log_reg_param_grid = [{\n",
        "    'estimator__C' : C\n",
        "}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9avHPfNr8A2m"
      },
      "source": [
        "### TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1_ZA11h8Btj"
      },
      "source": [
        "lr_tfidf = LogisticReg(log_reg_param_grid, tf_feature_train, y_train_ohe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUHo6sSY8KHJ"
      },
      "source": [
        "y_pred = lr_tfidf.predict(tf_feature_test)\n",
        "print(f1_score(y_test_ohe, y_pred, average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3KFB2eN8Lei"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14EnDxkqc0XG"
      },
      "source": [
        "lr_w2v = LogisticReg(log_reg_param_grid, w2v_feature_train, y_train_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4najxLfGDao"
      },
      "source": [
        "y_pred = lr_w2v.predict(w2v_feature_test)\n",
        "print(f1_score(y_test_w2v, y_pred, average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oURBIBs_NPir"
      },
      "source": [
        "### ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2Vn2fpSNQo6"
      },
      "source": [
        "lr_elmo = LogisticReg(log_reg_param_grid, elmo_feature_train, y_train_ohe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7F-k8aCNmlz"
      },
      "source": [
        "y_pred = lr_elmo.predict(elmo_feature_test)\n",
        "print(f1_score(y_test_ohe, y_pred, average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-rJ5YXqc0kp"
      },
      "source": [
        "## Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCq2WIJUdBDb"
      },
      "source": [
        "nb_param_grid = [{  \n",
        "'estimator__alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001)  \n",
        "}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYu7kMlac3q-"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG9YkTu6c4xO"
      },
      "source": [
        "mnb_tfidf = MNB(nb_param_grid, tf_feature_train, y_train_ohe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76x5522wNT47"
      },
      "source": [
        "y_pred = mnb_tfidf.predict(tf_feature_test)\n",
        "print(f1_score(y_test_ohe,y_pred, average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO5UxhO4c49d"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-SkPhyfGJc0"
      },
      "source": [
        "mnb_w2v = MNB(nb_param_grid, w2v_feature_train, y_train_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwu8p0Aac6Ey"
      },
      "source": [
        "y_pred = mnb_w2v.predict(w2v_feature_test)\n",
        "print(f1_score(y_test_w2v, y_pred, average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcXs5_IHNsgS"
      },
      "source": [
        "### ElMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XRaw65ONwFy"
      },
      "source": [
        "mnb_w2v = MNB(nb_param_grid, elmo_feature_train, y_train_ohe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9tlD1aINtky"
      },
      "source": [
        "y_pred = mnb_elmo.predict(tf_feature_test)\n",
        "print(f1_score(y_test_ohe, y_pred, average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKrpInCEc6Vx"
      },
      "source": [
        "## SVM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuEDAEyqc90r"
      },
      "source": [
        "svm_param_grid = [{'C': [0.1,1, 10, 100], \n",
        "                   'gamma': [1,0.1,0.01,0.001]}\n",
        "                  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpWA_3R7c865"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCepZV8yOfdf"
      },
      "source": [
        "svm_tfidf = SVM(svm_param_grid, tf_feature_train, y_train_ohe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZH7MyOlOlYp"
      },
      "source": [
        "y_pred = svm_tfidf.predict(tf_feature_test)\n",
        "print(f1_score(y_test_ohe,y_pred,average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2xWt9n5c990"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSUhbidoc_Ka"
      },
      "source": [
        "svm_w2v = SVM(svm_param_grid, w2v_feature_train, y_train_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpuNv1fnGTx2"
      },
      "source": [
        "y_pred = svm_tfidf.predict(w2v_feature_test)\n",
        "print(f1_score(y_test_w2v,y_pred,average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7kLWgEYNz-I"
      },
      "source": [
        "### ElMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkfvfQPjN0-U"
      },
      "source": [
        "svm_elmo = SVM(svm_param_grid, elmo_feature_train, y_train_ohe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUQSXSf5N5mr"
      },
      "source": [
        "y_pred = svm_elmo.predict(elmo_feature_test)\n",
        "print(f1_score(y_test_w2v,y_pred,average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}